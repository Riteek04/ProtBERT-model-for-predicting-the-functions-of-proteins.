{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers biopython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from Bio import SeqIO\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load ProtBERT Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "model = BertModel.from_pretrained(\"Rostlab/prot_bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. FASTA Parsing and Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse FASTA file\n",
    "def parse_fasta(fasta_path):\n",
    "    sequences = []\n",
    "    for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "        seq = str(record.seq)\n",
    "        if set(seq).issubset(set(\"ACDEFGHIKLMNPQRSTVWY\")):\n",
    "            sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "# Preprocess sequence: add spaces between amino acids\n",
    "def preprocess_sequence(sequence):\n",
    "    return ' '.join(list(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sequences\n",
    "def tokenize_sequences(sequences, tokenizer, max_length=512):\n",
    "    preprocessed = [preprocess_sequence(seq) for seq in sequences]\n",
    "    return tokenizer(\n",
    "        preprocessed,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. PyTorch Dataset for Protein Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, sequences, tokenizer):\n",
    "        self.data = tokenize_sequences(sequences, tokenizer)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data['input_ids'].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.data['input_ids'][idx],\n",
    "            'attention_mask': self.data['attention_mask'][idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Upload and Read FASTA File in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "fasta_filename = next(iter(uploaded))\n",
    "sequences = parse_fasta(fasta_filename)\n",
    "sequences = sequences[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Create Dataset and Test One Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse FASTA file\n",
    "def parse_fasta(fasta_path):\n",
    "    sequences = []\n",
    "    for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "        seq = str(record.seq)\n",
    "        # Check if sequence contains only valid amino acids and is not empty\n",
    "        if seq and set(seq).issubset(set(\"ACDEFGHIKLMNPQRSTVWY\")):\n",
    "            sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "# Preprocess sequence: add spaces between amino acids\n",
    "def preprocess_sequence(sequence):\n",
    "    return ' '.join(list(sequence))\n",
    "\n",
    "# Tokenize sequences\n",
    "def tokenize_sequences(sequences, tokenizer, max_length=512):\n",
    "    preprocessed = [preprocess_sequence(seq) for seq in sequences]\n",
    "    # Add error handling for empty sequences\n",
    "    if not preprocessed:\n",
    "        return tokenizer.encode_plus(\"\", return_tensors=\"pt\")  # Handle empty sequence\n",
    "    else:\n",
    "        return tokenizer(\n",
    "            preprocessed,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Run Sample Through ProtBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through ProtBERT\n",
    "# Create dataset instance\n",
    "dataset = ProteinDataset(sequences, tokenizer)\n",
    "\n",
    "# Get a sample from the dataset\n",
    "sample = dataset[0]  # Get the first sample\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(\n",
    "        input_ids=sample['input_ids'].unsqueeze(0),\n",
    "        attention_mask=sample['attention_mask'].unsqueeze(0)\n",
    "    )\n",
    "\n",
    "print(\"Output shape:\", output.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Visualize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Mean pooling\n",
    "mean_embedding = output.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "sns.barplot(x=list(range(50)), y=mean_embedding[:50].numpy())\n",
    "plt.title(\"First 50 dimensions of ProtBERT embedding\")\n",
    "plt.xlabel(\"Embedding dimension\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "# Load the model and move to the appropriate device\n",
    "model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "model = model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "# Load the model and move to the appropriate device\n",
    "model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "model = model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "# Load the model and move to the appropriate device\n",
    "model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "model = model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "# Load the model and move to the appropriate device\n",
    "model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "model = model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming `ProteinDataset` and `sequences` are defined in previous cells\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Install + Load ProtBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install scikit-learn\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load ProtBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "model.eval()\n",
    "\n",
    "# Preprocess sequences\n",
    "def preprocess_sequence(seq):\n",
    "    return ' '.join(list(seq)).upper()\n",
    "\n",
    "def extract_embedding(sequence):\n",
    "    tokens = tokenizer(sequence, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    return output.last_hidden_state.mean(dim=1).squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Protein Function Prediction (GO/EC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example: Duplicate the small dataset to simulate more data (for testing purpose only)\n",
    "sequences = [\"MKWVTFISLLFLFSSAYSR\", \"GVFRRDTHKSEIAHRF\"] * 20  # 40 sequences\n",
    "labels = [1, 0] * 20  # 40 labels\n",
    "\n",
    "X = [extract_embedding(preprocess_sequence(seq)) for seq in sequences]\n",
    "y = np.array(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Subcellular Localization Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequences labeled with subcellular location (e.g., cytoplasm = 0, nucleus = 1)\n",
    "sequences = [\"MALWMRLLPLL\", \"MNIFEMLRID\"]\n",
    "labels = [0, 1]\n",
    "\n",
    "X = [extract_embedding(preprocess_sequence(seq)) for seq in sequences]\n",
    "y = np.array(labels)\n",
    "\n",
    "# Instead of StratifiedKFold or LeaveOneOut, use k-Nearest Neighbors for small datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create and train the classifier\n",
    "clf = KNeighborsClassifier(n_neighbors=1) # Using 1 neighbor for this small dataset\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Predict on the entire dataset (as we used all data for training)\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Protein-Protein Interaction (PPI) Prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairs of sequences + binary label (1: interact, 0: no interaction)\n",
    "pairs = [(\"MKWVTFISLL\", \"GVFRRDTHKS\"), (\"MNIFEMLRID\", \"MALWMRLLPL\")]\n",
    "labels = [1, 0]\n",
    "\n",
    "X = []\n",
    "for seq1, seq2 in pairs:\n",
    "    emb1 = extract_embedding(preprocess_sequence(seq1))\n",
    "    emb2 = extract_embedding(preprocess_sequence(seq2))\n",
    "    pair_emb = np.concatenate([emb1, emb2])  # Concatenate embeddings\n",
    "    X.append(pair_emb)\n",
    "\n",
    "y = np.array(labels)\n",
    "\n",
    "# Instead of train_test_split and LogisticRegression, use KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=1)  # Using 1 neighbor for this small dataset\n",
    "clf.fit(X, y)  # Train on the entire dataset\n",
    "\n",
    "# Predict on the entire dataset (as we used all data for training)\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Protein Structure Prediction (2D/3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each sequence has a label per residue (e.g., H/E/C) â€” simplified here\n",
    "sequences = [\"MKWVTF\", \"GVFRRD\"]\n",
    "labels = [\"HHHEEE\", \"EECCCC\"]  # 1 label per amino acid\n",
    "\n",
    "# Token-level prediction: convert each amino acid to embedding (averaged over window)\n",
    "def extract_token_embeddings(sequence):\n",
    "    tokens = tokenizer(preprocess_sequence(sequence), return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    # Remove special tokens ([CLS], [SEP]) before returning embeddings\n",
    "    return output.last_hidden_state[0, 1:-1, :].numpy() # shape: (L, hidden_dim)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for seq, lbl_seq in zip(sequences, labels):\n",
    "    emb = extract_token_embeddings(seq)\n",
    "    # Ensure the number of embeddings and labels match\n",
    "    # by trimming the longer one to match the shorter\n",
    "    min_len = min(len(emb), len(lbl_seq))\n",
    "    X.extend(emb[:min_len])\n",
    "    y.extend(lbl_seq[:min_len])  # keep per-token labels\n",
    "\n",
    "# Encode labels (H = 0, E = 1, C = 2)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X, y_encoded)\n",
    "\n",
    "print(\"Accuracy:\", clf.score(X, y_encoded))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
