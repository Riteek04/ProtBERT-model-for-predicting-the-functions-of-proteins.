# ðŸ§¬ Deep Learning on Protein Sequences using ProtBERT

## ðŸ“Œ Project Overview

This project leverages **ProtBERT**, a deep learning model inspired by BERT, to understand and analyze **protein sequences** using techniques from **Natural Language Processing (NLP)**. ProtBERT treats protein sequences like text â€” where amino acids act like words â€” enabling powerful predictions and classifications in biological data.

## ðŸ‘¨â€ðŸ”¬ Team Members

- Anshuman Rath â€“ 22BAC10002  
- Pratham Verma â€“ 22BAC10005  
- Riteek Panigrahi â€“ 22BAC10012  
- M K Kiriti â€“ 22BAC10016  
- Priyanshu Kumar Singh â€“ 22BEC10006  
- Aman Singh â€“ 22BEC10010  

## ðŸ” Why ProtBERT?

ProtBERT applies NLP-based transformer models to protein sequences, allowing us to:

- ðŸ”¬ **Classify proteins** based on function  
- ðŸ§² **Identify binding sites** for drug interaction  
- ðŸ§¬ **Predict subcellular localization** (e.g., nucleus, mitochondria, cytoplasm)  
- ðŸ“ˆ Improve protein function analysis using deep learning

## ðŸ§± Protein Sequences & Amino Acids

- Proteins are composed of 20 amino acids, each with a unique side chain.
- Example sequence: `MKTAYIAKQRQISF`
- Each letter represents a specific amino acid.
- Understanding these sequences allows for insights into **function**, **structure**, and **disease targeting**.

## ðŸ¤– How ProtBERT Works

ProtBERT is based on the **BERT architecture**:
- **Word Embeddings** â†’ Represent amino acids as vectors  
- **Positional Encoding** â†’ Maintains sequence order  
- **Self-Attention Layers** â†’ Learns relationships between amino acids  
- **Masked Language Modeling** â†’ Trains by hiding random amino acids and predicting them

## ðŸ“š Dataset Information

- Source: **[UniProt](https://www.uniprot.org/)** â€“ a widely used protein sequence database
- Format: **FASTA**
  - Header line (starts with `>`) contains metadata
  - Following lines represent the amino acid sequence

### ðŸ” FASTA Example
```text
>sp|A0A087X1C5|CP2D7_HUMAN Putative cytochrome P450 2D7 OS=Homo sapiens OX=9606 GN=CYP2D7 PE=5 SV=1  
MGLEALVPLAMIVAIFLLLVDLMHRHQRWAARYPPGPLPLPGLGNLLHVDFQNTPYCFDQâ€¦
```

- `sp` â†’ Swiss-Prot (manually curated)  
- `A0A087X1C5` â†’ Accession number  
- `CP2D7_HUMAN` â†’ Protein name  
- `OS=Homo sapiens` â†’ Organism source  
- `GN=CYP2D7` â†’ Gene name  
- `PE=5` â†’ Protein existence level  
- `SV=1` â†’ Sequence version  

## ðŸš€ Future Scope

| Technique | Description |
|----------|-------------|
| Enhanced Predictive Accuracy | Training on larger datasets |
| Integration with Omics | Combine protein data with genomics, transcriptomics, etc. |
| Multi-Modal Training | Use structural + sequence data |
| Improved Interpretability | Add visualization tools for predictions |

## ðŸ™Œ Thank You!

This project showcases how **transformer-based models** can revolutionize **bioinformatics**, opening new doors in drug discovery, genomics, and molecular biology.
